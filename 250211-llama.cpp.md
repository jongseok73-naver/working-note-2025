

Runing LLaMA.cpp with CPU only on my x300 mini-PC at home..

 
## How to download gguf

1. visit huggingface and download on Webpage 
2. wget like this
```
wget https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf
```
popular contributers share their gguf models at huggingface.

  
## How to run llama.cpp bench
```
./llama-bench -m ~/gguf_model/Llama-3.2-1B-Instruct-Q4_K_M.gguf -n 128 -p 512,4096 -pg 4096,128 -ngl 99 -r 2
```

And you can see the result : 
```
| model                          |       size |     params | backend    | threads |          test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
| llama 1B Q4_K - Medium         | 762.81 MiB |     1.24 B | CPU        |       8 |         pp512 |        191.86 ± 0.53 |
| llama 1B Q4_K - Medium         | 762.81 MiB |     1.24 B | CPU        |       8 |        pp4096 |        143.42 ± 0.23 |
| llama 1B Q4_K - Medium         | 762.81 MiB |     1.24 B | CPU        |       8 |         tg128 |         34.71 ± 0.37 |
| llama 1B Q4_K - Medium         | 762.81 MiB |     1.24 B | CPU        |       8 |  pp4096+tg128 |        124.47 ± 2.14 |
```


## How to run llama.cpp on webui

please visit : 
https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/

```
./llama-server -m ~/gguf_model/Llama-3.2-1B-Instruct-Q4_K_M.gguf --port 10000 --ctx-size 1024
```
and run webbrowser like firefox :  http://127.0.0.1:10000
![Screenshot from 2025-02-11 04-18-12](https://github.com/user-attachments/assets/c3ca3b87-287e-4400-a38b-59adce68630b)
